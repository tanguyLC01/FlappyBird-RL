{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    action = env.action_space.sample()  # env.action_space.sample() for a random action\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Rendering the game:\n",
    "    # (remove this two lines during training)\n",
    "    env.render()\n",
    "    time.sleep(1 / 30)  # FPS\n",
    "    \n",
    "    # Checking if the player is still alive\n",
    "    if done:\n",
    "        break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(6 * 6 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, action_space)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, observation):\n",
    "        output = self.relu(self.conv1(observation))\n",
    "        output = self.relu(self.conv2(output))\n",
    "        output = self.relu(self.conv3(output))\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.eps_decay = parameters['eps_decay']\n",
    "        self.eps_min = parameters['eps_min']\n",
    "        self.eps = parameters['eps_start']\n",
    "        self.action_space = parameters['action_space']\n",
    "        self.memory = deque(maxlen=parameters['memory_size'])\n",
    "        self.device = parameters['device']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.Q_policy = DQN(self.action_space).to(self.device)\n",
    "        self.Q_target = copy.deepcopy(self.Q_policy).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.Q_policy.parameters(), lr = parameters['learning_rate'])\n",
    "        self.lossFuc = torch.nn.MSELoss()\n",
    "        self.action_dict = {0:0, 1:1}\n",
    "        \n",
    "    def one_hot_embedding(labels, num_classes):\n",
    "        \"\"\"Embedding labels to one-hot form.\n",
    "        Args:\n",
    "        labels: (LongTensor) class labels, sized [N,].\n",
    "        num_classes: (int) number of classes.\n",
    "\n",
    "        Returns:\n",
    "        (tensor) encoded labels, sized [N, #classes].\n",
    "        \"\"\"\n",
    "        y = torch.eye(num_classes) \n",
    "        return y[labels] \n",
    "    \n",
    "    def img_process(self, img):\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        img = cv2.flip(img,1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = img[:400,:]\n",
    "        img = cv2.resize(img, (80, 80))\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        retval, img = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)\n",
    "        #img = torch.FloatTensor(img)\n",
    "        return img\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def predict(self, state):\n",
    "        state = np.reshape(state, (1, 1, 80, 80))  # Reshape to (batch_size, channels, height, width)\n",
    "        state_tensor = torch.FloatTensor(state)  \n",
    "        q_values = self.Q_policy(state_tensor)[0]\n",
    "\n",
    "        # best action\n",
    "        if np.random.rand() < self.eps:\n",
    "            max_q_index = [random.randrange(self.action_space)]\n",
    "            max_q_one_hot =  self.one_hot_embedding(max_q_index, self.action_space)\n",
    "        else:\n",
    "            max_q_index =  [torch.max(q_values, 0).indices.cpu().numpy().tolist()]\n",
    "            max_q_one_hot =  self.one_hot_embedding(max_q_index, self.action_space)\n",
    "       # print(torch.max(q_values).detach().numpy())\n",
    "        return max_q_index, max_q_one_hot, q_values\n",
    "        \n",
    "    def experince_replay(self):\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            batch = random.sample(self.memory, self.batch_size) \n",
    "            state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = zip(*batch)\n",
    "            state_batch = torch.cat(tuple(torch.FloatTensor(state) for state in state_batch)).reshape(self.batch_size, 1, 80, 80)\n",
    "            action_batch = torch.cat(tuple(torch.FloatTensor(action) for action in action_batch)).reshape(self.batch_size, self.action_space)\n",
    "            reward_batch = torch.cat(tuple(torch.Tensor(reward) for reward in reward_batch)).reshape(self.batch_size, 1)\n",
    "            next_state_batch = torch.cat(tuple(torch.Tensor(next_state) for next_state in next_state_batch)).reshape(self.batch_size, 1, 80, 80)\n",
    "            current_prediction_batch = self.Q_policy(state_batch)\n",
    "            next_prediction_batch = self.Q_target(next_state_batch)\n",
    "\n",
    "            y_batch = torch.cat(\n",
    "                tuple(reward if terminal else reward + self.gamma * torch.max(prediction) for reward, terminal, prediction in\n",
    "                    zip(reward_batch, terminal_batch, next_prediction_batch)))\n",
    "\n",
    "            q_value = torch.sum(current_prediction_batch * action_batch, dim=1)\n",
    "            self.optimizer.zero_grad()\n",
    "            # y_batch = y_batch.detach()\n",
    "            loss = self.lossFuc(q_value, y_batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.exploration_rate  = max(self.eps_min, self.eps_decay * self.eps)\n",
    "        \n",
    "    def train(self, env):    \n",
    "        reward = 0.0\n",
    "        run = 0\n",
    "        score = []\n",
    "        q_values = []\n",
    "        while True:\n",
    "            run += 1\n",
    "            state = env.reset()\n",
    "            state = self.img_process(state)\n",
    "            step = 0\n",
    "            if run % 5000 == 4999:\n",
    "                torch.save(self.Q_policy.state_dict(), 'model.pth')\n",
    "            done =  False\n",
    "            while not done:\n",
    "                step += 1\n",
    "                action, action_one_hot, p_values = self.predict(state)\n",
    "                next_state, reward, done, info = env.step(self.action_dict[action[0]])\n",
    "                state_next = self.img_process(next_state)\n",
    "                \n",
    "                reward = torch.tensor([reward])\n",
    "                if torch.cuda.is_available():\n",
    "                    reward = reward.cuda()\n",
    "                    action_one_hot = action_one_hot.cuda()\n",
    "                self.remember(state, action_one_hot, reward, state_next,\n",
    "                                    done)\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    print (\"Run: \" + str(run) + \", exploration: \" + str(self.eps) + \", score: \" + str(step) )\n",
    "                    print('Q_value :', torch.max(p_values))\n",
    "                    q_value.append(torch.max(p_values))\n",
    "                    score.append(step)\n",
    "                    print('Score : ', step)\n",
    "                    break\n",
    "                self.experince_replay()\n",
    "                if step % 50 == 0:\n",
    "                    self.Q_target.load_state_dict(copy.deepcopy(self.Q_policy.state_dict()))\n",
    "        return score, q_values\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_hot_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m env \u001b[38;5;241m=\u001b[39m flappy_bird_gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlappyBird-rgb-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m dql \u001b[38;5;241m=\u001b[39m Agent(parameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n\u001b[1;32m---> 25\u001b[0m q_values, score \u001b[38;5;241m=\u001b[39m \u001b[43mdql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 105\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    104\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 105\u001b[0m     action, action_one_hot, p_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dict[action[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m    107\u001b[0m     state_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_process(next_state)\n",
      "Cell \u001b[1;32mIn[9], line 61\u001b[0m, in \u001b[0;36mAgent.predict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     59\u001b[0m  \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m      max_q_index \u001b[38;5;241m=\u001b[39m  [torch\u001b[38;5;241m.\u001b[39mmax(q_values, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m---> 61\u001b[0m      max_q_one_hot \u001b[38;5;241m=\u001b[39m  \u001b[43mone_hot_embedding\u001b[49m(max_q_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# print(torch.max(q_values).detach().numpy())\u001b[39;00m\n\u001b[0;32m     63\u001b[0m  \u001b[38;5;28;01mreturn\u001b[39;00m max_q_index, max_q_one_hot, q_values\n",
      "\u001b[1;31mNameError\u001b[0m: name 'one_hot_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'eps_decay': 0.999,\n",
    "    'eps_start': 0.1, \n",
    "    'eps_min': 0.01,\n",
    "    'action_space': 2,\n",
    "    'memory_size': 10000,\n",
    "    'learning_rate': 1e-4,\n",
    "    'gamma': 0.9,\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'pre_train_steps': 1000,\n",
    "    'num_episodes': 10000,\n",
    "    'batch_size': 32,\n",
    "    'update_freq': 1000,\n",
    "    'img_width': 80,\n",
    "    'img_height': 80,\n",
    "    'state_size': 6400,\n",
    "    'action_size': 2,\n",
    "    'img_buffer_size': 50000\n",
    "}\n",
    "\n",
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "\n",
    "dql = Agent(parameters=hyperparameters)\n",
    "q_values, score = dql.train(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
