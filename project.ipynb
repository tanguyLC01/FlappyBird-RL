{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    action = env.action_space.sample()  # env.action_space.sample() for a random action\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Rendering the game:\n",
    "    # (remove this two lines during training)\n",
    "    env.render()\n",
    "    time.sleep(1 / 30)  # FPS\n",
    "    \n",
    "    # Checking if the player is still alive\n",
    "    if done:\n",
    "        break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(6 * 6 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, action_space)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, observation):\n",
    "        output = self.relu(self.conv1(observation))\n",
    "        output = self.relu(self.conv2(output))\n",
    "        output = self.relu(self.conv3(output))\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.eps_decay = parameters['eps_decay']\n",
    "        self.eps_min = parameters['eps_min']\n",
    "        self.eps = parameters['eps_start']\n",
    "        self.action_space = parameters['action_space']\n",
    "        self.memory = deque(maxlen=parameters['memory_size'])\n",
    "        self.device = parameters['device']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.Q_policy = DQN(self.action_space).to(self.device)\n",
    "        self.Q_target = copy.deepcopy(self.Q_policy).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.Q_policy.parameters(), lr = parameters['learning_rate'])\n",
    "        self.lossFuc = torch.nn.MSELoss()\n",
    "        self.action_dict = {0:0, 1:1}\n",
    "        \n",
    "    def one_hot_embedding(labels, num_classes):\n",
    "        \"\"\"Embedding labels to one-hot form.\n",
    "        Args:\n",
    "        labels: (LongTensor) class labels, sized [N,].\n",
    "        num_classes: (int) number of classes.\n",
    "\n",
    "        Returns:\n",
    "        (tensor) encoded labels, sized [N, #classes].\n",
    "        \"\"\"\n",
    "        y = torch.eye(num_classes) \n",
    "        return y[labels] \n",
    "    \n",
    "    def img_process(self, img):\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        img = cv2.flip(img,1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = img[:400,:]\n",
    "        img = cv2.resize(img, (80, 80))\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        retval, img = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)\n",
    "        #img = torch.FloatTensor(img)\n",
    "        return img\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def predict(self, state):\n",
    "        state = np.reshape(state, (1, 1, 80, 80))  # Reshape to (batch_size, channels, height, width)\n",
    "        state_tensor = torch.FloatTensor(state)  \n",
    "        q_values = self.Q_policy(state_tensor)[0]\n",
    "\n",
    "        # best action\n",
    "        if np.random.rand() < self.eps:\n",
    "            max_q_index = [random.randrange(self.action_space)]\n",
    "            max_q_one_hot =  one_hot_embedding(max_q_index, self.action_space)\n",
    "        else:\n",
    "            max_q_index =  [torch.max(q_values, 0).indices.cpu().numpy().tolist()]\n",
    "            max_q_one_hot =  one_hot_embedding(max_q_index, self.action_space)\n",
    "       # print(torch.max(q_values).detach().numpy())\n",
    "        return max_q_index, max_q_one_hot, q_values\n",
    "        \n",
    "    def experince_replay(self):\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "            batch = random.sample(self.memory, self.batch_size) \n",
    "            state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = zip(*batch)\n",
    "            state_batch = torch.cat(tuple(torch.FloatTensor(state) for state in state_batch)).reshape(self.batch_size, 1, 80, 80)\n",
    "            action_batch = torch.cat(tuple(torch.FloatTensor(action) for action in action_batch)).reshape(self.batch_size, self.action_space)\n",
    "            reward_batch = torch.cat(tuple(torch.Tensor(reward) for reward in reward_batch)).reshape(self.batch_size, 1)\n",
    "            next_state_batch = torch.cat(tuple(torch.Tensor(next_state) for next_state in next_state_batch)).reshape(self.batch_size, 1, 80, 80)\n",
    "            current_prediction_batch = self.Q_policy(state_batch)\n",
    "            next_prediction_batch = self.Q_target(next_state_batch)\n",
    "\n",
    "            y_batch = torch.cat(\n",
    "                tuple(reward if terminal else reward + self.gamma * torch.max(prediction) for reward, terminal, prediction in\n",
    "                    zip(reward_batch, terminal_batch, next_prediction_batch)))\n",
    "\n",
    "            q_value = torch.sum(current_prediction_batch * action_batch, dim=1)\n",
    "            self.optimizer.zero_grad()\n",
    "            # y_batch = y_batch.detach()\n",
    "            loss = self.lossFuc(q_value, y_batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.exploration_rate  = max(self.eps_min, self.eps_decay * self.eps)\n",
    "        \n",
    "    def train(self, env):    \n",
    "        reward = 0.0\n",
    "        run = 0\n",
    "        while True:\n",
    "            run += 1\n",
    "            state = env.reset()\n",
    "            state = self.img_process(state)\n",
    "            step = 0\n",
    "            if run % 5000 == 4999:\n",
    "                torch.save(self.Q_policy.state_dict(), Model_path)\n",
    "            done =  False\n",
    "            while not done:\n",
    "                step += 1\n",
    "                action, action_one_hot, p_values = self.predict(state)\n",
    "                next_state, reward, done, info = env.step(self.action_dict[action[0]])\n",
    "                state_next = self.img_process(next_state)\n",
    "                \n",
    "                reward = torch.tensor([reward])\n",
    "                if torch.cuda.is_available():\n",
    "                    reward = reward.cuda()\n",
    "                    action_one_hot = action_one_hot.cuda()\n",
    "                self.remember(state, action_one_hot, reward, state_next,\n",
    "                                    done)\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    print (\"Run: \" + str(run) + \", exploration: \" + str(self.eps) + \", score: \" + str(step) )\n",
    "                    print('Q_value :', torch.max(p_values))\n",
    "                    print('Score : ', step)\n",
    "                    break\n",
    "                self.experince_replay()\n",
    "                if step % 50 == 0:\n",
    "                    self.Q_target.load_state_dict(copy.deepcopy(self.Q_policy.state_dict()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 0.1, score: 101\n",
      "Q_value : tensor(5.3833, grad_fn=<MaxBackward1>)\n",
      "Score :  101\n",
      "Run: 2, exploration: 0.1, score: 101\n",
      "Q_value : tensor(6.4129, grad_fn=<MaxBackward1>)\n",
      "Score :  101\n",
      "Run: 3, exploration: 0.1, score: 101\n",
      "Q_value : tensor(7.4474, grad_fn=<MaxBackward1>)\n",
      "Score :  101\n",
      "Run: 4, exploration: 0.1, score: 101\n",
      "Q_value : tensor(8.0958, grad_fn=<MaxBackward1>)\n",
      "Score :  101\n",
      "Run: 5, exploration: 0.1, score: 101\n",
      "Q_value : tensor(8.6317, grad_fn=<MaxBackward1>)\n",
      "Score :  101\n",
      "Run: 6, exploration: 0.1, score: 101\n",
      "Q_value : tensor(9.3733, grad_fn=<MaxBackward1>)\n",
      "Score :  101\n",
      "Run: 7, exploration: 0.1, score: 101\n",
      "Q_value : tensor(9.0553, grad_fn=<MaxBackward1>)\n",
      "Score :  101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m env \u001b[38;5;241m=\u001b[39m flappy_bird_gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlappyBird-rgb-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m dql \u001b[38;5;241m=\u001b[39m Agent(parameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mdql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[157], line 119\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore : \u001b[39m\u001b[38;5;124m'\u001b[39m, step)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperince_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_target\u001b[38;5;241m.\u001b[39mload_state_dict(copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_policy\u001b[38;5;241m.\u001b[39mstate_dict()))\n",
      "Cell \u001b[1;32mIn[157], line 84\u001b[0m, in \u001b[0;36mAgent.experince_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# y_batch = y_batch.detach()\u001b[39;00m\n\u001b[0;32m     83\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlossFuc(q_value, y_batch)\n\u001b[1;32m---> 84\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps_decay \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\RL\\project\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\RL\\project\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\RL\\project\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'eps_decay': 0.999,\n",
    "    'eps_start': 0.1, \n",
    "    'eps_min': 0.01,\n",
    "    'action_space': 2,\n",
    "    'memory_size': 10000,\n",
    "    'learning_rate': 1e-4,\n",
    "    'gamma': 0.9,\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'pre_train_steps': 1000,\n",
    "    'num_episodes': 10000,\n",
    "    'batch_size': 32,\n",
    "    'update_freq': 1000,\n",
    "    'img_width': 80,\n",
    "    'img_height': 80,\n",
    "    'state_size': 6400,\n",
    "    'action_size': 2,\n",
    "    'img_buffer_size': 50000\n",
    "}\n",
    "\n",
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "\n",
    "dql = Agent(parameters=hyperparameters)\n",
    "dql.train(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
